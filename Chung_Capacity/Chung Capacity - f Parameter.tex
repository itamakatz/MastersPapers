\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{amsmath}
\usepackage{IEEEtrantools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{sectsty}
\usepackage{xstring}
\usepackage{xparse}

\usepackage{mleftright}
\mleftright

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}

\usepackage{cancel}

\interdisplaylinepenalty=100
% \interdisplaylinepenalty=10
% \interdisplaylinepenalty=0

\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\makeatletter
\def\IEEElabelanchoreqn#1{\bgroup
\def\@currentlabel{\p@equation\theequation}\relax
\def\@currentHref{\@IEEEtheHrefequation}\label{#1}\relax
\Hy@raisedlink{\hyper@anchorstart{\@currentHref}}\relax
\Hy@raisedlink{\hyper@anchorend}\egroup}
\makeatother
\newcommand{\subnumberinglabel}[1]{\IEEEyesnumber
\IEEEyessubnumber*\IEEElabelanchoreqn{#1}}

\usepackage[dvipsnames]{xcolor}

\usepackage[bottom=1in,top=1in]{geometry}

\graphicspath{ {./images/} }
\iffalse
	\sectionfont{\fontsize{16}{15}\selectfont}
\fi

% *** Magic Commands! *** %

	% call empty: \coherent[]{S}
	% call with index: \coherent[0]{S+l}
	\NewDocumentCommand\coherent{om}{%
		\IfNoValueTF{#1}
		{$|#2\rangle$}
		{$|#2_#1\rangle$}%
	}

    \newcommand{\logFrac}[2]{\log{\left[\frac{#1}{#2}\right]}}
	\newcommand{\logH}[1]{#1\log{\left[\frac{1}{#1}\right]}}
	\newcommand{\logHminus}[1]{#1\log{#1}}

	\newcommand{\lambdaWave}{\overline{\lambda}}
	\newcommand{\curlyBracket}[1]{
		\begin{equation*}
			\left\{ \,
			\begin{IEEEeqnarraybox}[][c]{l"s}
				\IEEEstrut
					#1
				\IEEEstrut
			\end{IEEEeqnarraybox}
			\right.
		\end{equation*}
	}
	\newcommand{\curlyBracketFull}[2]{
		\begin{equation*}
			#1 \left\{ \,
			\begin{IEEEeqnarraybox}[][c]{l"s}
				\IEEEstrut
					#2
				\IEEEstrut
			\end{IEEEeqnarraybox}
			\right.
		\end{equation*}
	}
	\newcommand{\lambdaSwitch}[1]{%
		\IfEqCase{#1}{%
			{0}{\lambda_0=|S_0+l|^2}%
			{1}{\lambda_1=|S_1+l|^2}%
			{i}{\lambda_i=|S_i+l|^2}%
			{~}{\widetilde{\lambda}=\pi_{0}\sqrt{\lambda_{0}}+\pi_{1}\sqrt{\lambda_{1}}}%
			{-}{\lambdaWave=\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}}%
		}[\PackageError{tree}{Undefined option to tree: #1}{}]%
	}%

	
	\newcommand{\g}{g = \frac{\pi_{0}}{\pi_{1}}}
	\newcommand{\lStar}{\frac{S_{0}\pi_{0} - S_{1}\pi_{1}}{\pi_{1}-\pi_{0}}}
	
	\newcommand{\piG}[1]{%
		\IfEqCase{#1}{%
			{0}{\frac{g}{1+g}}%
			{1}{\frac{1}{1+g}}%
		}[\PackageError{tree}{Undefined option to tree: #1}{}]%
	}%

    \newcommand{\f}{f(t) = \pi_{0}(t)*\pi_{1}(t)}

	\newcommand{\piF}[1]{%
		\IfEqCase{#1}{%
			{0}{\frac{1+\sqrt{1-4f(t)}}{2}}%
			{1}{\frac{1-\sqrt{1-4f(t)}}{2}}%
		}[\PackageError{tree}{Undefined option to tree: #1}{}]%
	}%
	
	\newcommand{\inPiF}{\sqrt{1-4f(t)}}
	
% *********************** %

	\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
	\let\endchangemargin=\endlist 

	\newlength{\eqboxstorage}
	\newcommand{\eqbox}[1]{
		\setlength{\eqboxstorage}{\fboxsep}
		\setlength{\fboxsep}{6pt}
		\boxed{#1}
		\setlength{\fboxsep}{\eqboxstorage}
	}

	\newcommand{\dd}{\mathop{}\!\mathrm{d}}

    \newcommand{\starbreak}{%
        \fancybreak{* * *}%
    }

% *********************** %

\begin{document}

	\begin{center}
		\section*{Signal detection over an optical channel}
	\end{center}

	\vspace{0.3cm}

	Given an optical channel, a signal on that channel can be described as a coherent state denoted by \coherent{S} where  $S\in\mathbb{C}$
.\\\par
	Considering a Noise free environment, in order to detect a signal in this type of channel, 
	we use a photon counter receiver. Such a receiver is a direct detection receiver which detects 
	the intensity of the optical and generates a Poisson process, where the rate of the process holds $\lambda = |S|^2$ 
.\\\par
	Suppose we have two coherent state signals denoted by \coherent[0]{S}, \coherent[1]{S} and we would like to distinguish between the two 
	binary hypotheses with the corresponding priori probabilities $\pi_0, \pi_1$ respectively under hypotheses $H=0,1$, 
	while holding some transmission cost constraint
.\\\par
	One approach was given by \textbf{Kennedy} who proposed adding a constant additional coherent state signal \coherent{l}
	before feeding the signal’s input to the receiver. Doing so generates a coherent state \coherent{S+l} which 
	the receiver in turn outputs a Poisson process with rate $\lambdaSwitch{i}$
.\\\par
	An additional approach was given by \textbf{Dolinar} who suggested as continuation to Kennedy’s design to replace the constant 
	signal with a controlled signal \coherent{l(t)} which is chosen adaptively based on the photon arrivals up to that moment, 
	in order to achieve more certainty in the hypothesis choice with time
.\\\par
	The core concept of Dolinar’s updated design includes a recursive method in which the posterior probabilities of 
	the two possible hypotheses denoted by $\pi_1(t), \pi_2(t)$ are updated after each step of time $\Delta$ to yield 
	$\pi_1(t+\Delta), \pi_2(t+\Delta)$
.\\\par
	At this point, by making $\Delta$ arbitrarily small we can expect the current Poisson process to return
:\\	\curlyBracket{
		0 & w.p. $(1-\lambda_i\Delta)$
\\		1 & w.p. $(\lambda_i\Delta)$
	}
	Which can be thought of as the following binary channel (Figure \ref{fig:channel})
:\pagebreak[2]
	\begin{figure}[H]
		\centering
		\includegraphics[width=8cm]{channel.png}
		\caption{Equivalent binary channel over time $\Delta$}
		\label{fig:channel}
	\end{figure}
	Since we obtained an approximation of a binary channel we may now ask how should $l(t)$ be decided in 
	other to maximize the mutual information of the binary channel
.\\\par	The Entropy can be calculated as follows
:\\	\begin{IEEEeqnarray*}{r/C/l}
		H(Y) 	& = & H_{b}[\Delta(\pi_{0}\lambda_{0} + \pi_{1}\lambda_{1})]
\\		H(Y|X) 	& = & \pi_{0}H_{b}(\Delta\lambda_{1}) + \pi_{1}H_{b}(\Delta\lambda_{1})
	\end{IEEEeqnarray*}
\\	Which gives us the mutual information of the channel
:\\	\begin{IEEEeqnarray*}{r/C/l}
		I(X;Y) 	& = & H(Y)-H(Y|X)
\\				& = & H_{b}[\Delta(\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1})]-\left[\pi_{0}H_{b}(\Delta\lambda_{1})+\pi_{1}H_{b}(\Delta\lambda_{1})\right]
		\label{eq:dont_use_multline}
	\end{IEEEeqnarray*}
\\	Since we want to maximize the mutual information, we shall compare its derivative to zero
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\frac{dI(X;Y)}{dl} 	& = & \log_{2}\left[\frac{1-\Delta[\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}]}{\Delta[\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}]}\right]2\Delta(\pi_{0}\sqrt{\lambda_{0}}+\pi_{1}\sqrt{\lambda_{1}})
\\		&& -\> \log_{2}\left[\frac{1-\lambda_{0}\Delta}{\lambda_{0}\Delta}\right]2\Delta\pi_{0}\sqrt{\lambda_{0}}
\\		&& -\> \log_{2}\left[\frac{1-\lambda_{1}\Delta}{\lambda_{1}\Delta}\right]2\Delta\pi_{1}\sqrt{\lambda_{1}}
\\							& = & 0
	\end{IEEEeqnarray*}
	\begin{IEEEeqnarray*}{r/C/l}
		\frac{dI(X;Y)}{dl} 	& = & \log_{2}\left[\frac{1-\Delta[\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}]}{\Delta[\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}]}\right]2\Delta(\pi_{0}\sqrt{\lambda_{0}}+\pi_{1}\sqrt{\lambda_{1}})
\\		&& -\> 2\Delta\left[\log_{2}\left(\frac{1-\lambda_{0}\Delta}{\lambda_{0}\Delta}\right)\pi_{0}\sqrt{\lambda_{0}}+\log_{2}\left(\frac{1-\lambda_{1}\Delta}{\lambda_{1}\Delta}\right)\pi_{1}\sqrt{\lambda_{1}}\right]
\\							& = & 0
	\end{IEEEeqnarray*}
\\	The optimal choice for $l$ is
:\\	\begin{equation*}
		\eqbox{
			l^{*} = \lStar
		}
	\end{equation*}
\\	This optimal solution is numerically proven to be correct. The analytical proof will be included in a future paper.
	
\vspace{5mm}	
\centerline{* * *}
% \starbreak
\vspace{5mm}


In the optimal case where $l=l^{*}=\lStar$, we can calculate the following expressions for the $\lambda$'s
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\lambda_{i} & = & (l^{*}+S_{i})^{2}
\\					& = & \left(\lStar+S_{i}\right)^{2}
\\					& = & \left(\frac{S_{0}\pi_{0} - S_{1}\pi_{1}+S_{i}(\pi_{1}-\pi_{0})}{\pi_{1}-\pi_{0}}\right)^{2}
	\end{IEEEeqnarray*}
\\	Which yields:
\\	\begin{IEEEeqnarray*}{r/C/l}
		\overline{\lambda} 	& = & \pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}
\\							& = & \pi_{0}\left(\frac{\pi_{1}(S_{0}-S_{1})}{\pi_{1}-\pi_{0}}\right)^{2}
									+ \pi_{1}\left(\frac{\pi_{0}(S_{0}-S_{1})}{\pi_{1}-\pi_{0}}\right)^{2}
\\							& = & (\pi_{0}\pi_{1}^{2}+\pi_{1}\pi_{0}^{2})\left(\frac{(S_{0}-S_{1})}{\pi_{1}-\pi_{0}}\right)^{2}\IEEEyesnumber\label{eq:before_lambda_wave}
	\end{IEEEeqnarray*}
\\	Since $\pi_{0}+\pi_{1}=1$ we have that
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\pi_{0}\pi_{1}^{2}+\pi_{1}\pi_{0}^{2} 	& = & \pi_{0}\pi_{1}(1-\pi_{0}) +\pi_{1}\pi_{0}^{2}
\\												& = & \pi_{0}\pi_{1} - \pi_{0}\pi_{1}^{2} + \pi_{0}\pi_{1}^{2}
\\												& = & \pi_{0}\pi_{1}
	\end{IEEEeqnarray*}
\\	Thus we can substitute in (\ref{eq:before_lambda_wave}) and get
:\\	\begin{IEEEeqnarray*}{r/C/l}
		(\pi_{0}\pi_{1}^{2}+\pi_{1}\pi_{0}^{2})\left(\frac{S_{0}-S_{1}}{\pi_{1}-\pi_{0}}\right)^{2} & = & \pi_{0}\pi_{1}\left(\frac{S_{0}-S_{1}}{\pi_{1}-\pi_{0}}\right)^{2}
	\end{IEEEeqnarray*}
\\	Overall we get the following expression for the $\lambda$'s
:\\	\curlyBracket{
		\lambda_{0} = \left(\frac{\pi_{1}(S_{0}-S_{1})}{\pi_{1}-\pi_{0}}\right)^{2}
\\		\lambda_{1} = \left(\frac{\pi_{0}(S_{0}-S_{1})}{\pi_{1}-\pi_{0}}\right)^{2}
\\		\overline{\lambda} = \pi_{0}\pi_{1}\left(\frac{S_{0}-S_{1}}{\pi_{1}-\pi_{0}}\right)^{2}
	}
\\	At this point we can derive the mutual information in the optimal case
:\\	\begin{IEEEeqnarray*}{r/C/l}
		I 	& = & H(Y) - H(Y|X)
\\			& = & \textcolor{BrickRed}{H_{b}[\lambdaWave\Delta]-\pi_{0}H_{b}(\lambda_{0}\Delta)-\pi_{1}H_{b}(\lambda_{1}\Delta)}
\\			& = & \textcolor{BrickRed}{\logH{\lambdaWave}-\pi_{0}\logH{\lambda_{0}}-\pi_{1}\logH{\lambda_{1}}}
\\			& = & \logH{\pi_{0}\pi_{1}\left(\frac{S_{0}-S_{1}}{\pi_{1}-\pi_{0}}\right)^{2}}
\\			&	&		- \pi_{0}\logH{\left(\frac{\pi_{1}(S_{0}-S_{1})}{\pi_{1}-\pi_{0}}\right)^{2}}
\\			&	&		- \pi_{1}\logH{\left(\frac{\pi_{0}(S_{0}-S_{1})}{\pi_{1}-\pi_{0}}\right)^{2}}
\\			& = & \pi_{0}\pi_{1}\left(\frac{S_{0}-S_{1}}{\pi_{1}-\pi_{0}}\right)^{2}
\\			&	&		\left( 	- \log{\left[\pi_{0}\pi_{1}\left((\frac{(S_{0}-S_{1})}{\pi_{1}-\pi_{0}})\right)^{2}\right]}		\right.
\\			&	&				+\> \pi_{0}\log{\left[\left(\frac{\pi_{1}(S_{0}-S_{1})}{\pi_{1}-\pi_{0}})\right)^{2}\right]}
\\			&	&		\left. 	+\> \pi_{1}\log{\left[\left(\frac{\pi_{0}(S_{0}-S_{1})}{\pi_{1}-\pi_{0}})\right)^{2}\right]}	\right)
\\			& = & \pi_{0}\pi_{1}\left(\frac{S_{0}-S_{1}}{\pi_{1}-\pi_{0}}\right)^{2}
\\			&	&		\left(	- \log{[\pi_{0}\pi_{1}]} - \cancel{\log{\left[\left((\frac{(S_{0}-S_{1})}{\pi_{1}-\pi_{0}})\right)^{2}\right]}}				\right.
\\			&	&				+\> 2\pi_{0}\log{[\pi_{0}]} + \cancel{\pi_{0}\log{\left[\left(\frac{(S_{0}-S_{1})}{\pi_{1}-\pi_{0}})\right)^{2}\right]}}
\\			&	&		\left.	+\> 2\pi_{1}\log{[\pi_{1}]} + \cancel{\pi_{1}\log{\left[\left(\frac{(S_{0}-S_{1})}{\pi_{1}-\pi_{0}})\right)^{2}\right]}} 	\right)
\\			& = & \pi_{0}\pi_{1}\left(\frac{S_{0}-S_{1}}{\pi_{1}-\pi_{0}}\right)^{2} ( - log{[\pi_{0}\pi_{1}]} + 2\pi_{0}\log{[\pi_{0}]} + 2\pi_{1}\log{[\pi_{1}]})
\\			& = & \pi_{0}\pi_{1}\left(\frac{S_{0}-S_{1}}{\pi_{1}-\pi_{0}}\right)^{2} ( - log{[\pi_{0}]} - log{[\pi_{1}]} + 2\pi_{0}\log{[\pi_{0}]} + 2\pi_{1}\log{[\pi_{1}]})
\\			& = & \pi_{0}\pi_{1}\left(\frac{S_{0}-S_{1}}{\pi_{1}-\pi_{0}}\right)^{2} ( (2\pi_{0}-1)log{[\pi_{0}]} + (2\pi_{1}-1)log{[\pi_{1}]}) \IEEEyesnumber\label{eq:before_two_pi}
	\end{IEEEeqnarray*}
\\	Again, since $\pi_{0}+\pi_{1}=1$ we have
:\\	\curlyBracket{
		2\pi_{0}-1=\pi_{0}+\pi_{0}-1=\pi_{0}-\pi_{1}
\\		2\pi_{1}-1=\pi_{1}+\pi_{1}-1=\pi_{1}-\pi_{0}
	}
\\	So we can substitute in (\ref{eq:before_two_pi}) and get
:\\	\begin{IEEEeqnarray*}{r/C/l}
		& = & \pi_{0}\pi_{1}\frac{(S_{0}-S_{1})^{2}}{(\pi_{1}-\pi_{0})^{\cancel{2}}}\cancel{(\pi_{0}-\pi_{1})}(log{[\pi_{0}]}-log{[\pi_{1}]})
\\		& = & -\> \pi_{0}\pi_{1}\left(\frac{(S_{0}-S_{1})^{2}}{\pi_{1}-\pi_{0}}\right) log{\left[\frac{\pi_{0}}{\pi_{1}}\right]}
\\		& = & \pi_{0}\pi_{1}\left(\frac{(S_{0}-S_{1})^{2}}{\pi_{0}-\pi_{1}}\right) log{\left[\frac{\pi_{0}}{\pi_{1}}\right]}\IEEEyesnumber\label{eq:before_q}
	\end{IEEEeqnarray*}

At this point we shall parameterize our probabilities with parameter $\f$ which will let us describe the recursive procedure in the system. That definition yields the parametrization
:\\	\curlyBracketFull
    {\f \Longrightarrow}
    {
		\pi_{0}(t)=\piF{0}
\\		\pi_{1}(t)=\piF{1}
	}
	
With this new parameterization, we can now give an alternative continuation developing the Mutual Information we found in equation (\ref{eq:before_q})
:\\	\begin{IEEEeqnarray*}{r/C/l}
        I   & = & \pi_{0}\pi_{1}\left(\frac{(S_{0}-S_{1})^{2}}{\pi_{0}-\pi_{1}}\right) log{\left[\frac{\pi_{0}}{\pi_{1}}\right]}
\\          & = & f(t)\frac{(S_{0}-S_{1})^{2}}{\inPiF}\logFrac{1+\inPiF}{1-\inPiF}
	\end{IEEEeqnarray*}
	
As we did with the mutual information, we can now develop the Entropy of the channel and substitute with our current parameter of interest $f(t)$
:\\	\begin{IEEEeqnarray*}{r/C/l}
        H_b(\pi_0,\pi_1)    & = & \logH{\pi_0}+\logH{\pi_1}
\\                          & = & \logH{\pi_1}+(1-\pi_0)\logFrac{1}{\pi_1}
\\                          & = & \logFrac{1}{\pi_1}+\pi_0\logFrac{\pi_1}{\pi_0}
\\                          & = & \logFrac{2}{1-\inPiF}+\frac{1+\inPiF}{2}\logFrac{1-\inPiF}{1+\inPiF}
	\end{IEEEeqnarray*}
\\ This yields the following derivative of the entropy
:\\	\begin{IEEEeqnarray*}{r/C/l}
        \frac{dH(f(t))}{df(t)}    & = & -\frac{\logFrac{1-\inPiF}{1+\inPiF}}{\inPiF}
	\end{IEEEeqnarray*}
	
\pagebreak[2]

Each step of the recursion, the current mutual information gives us more knowledge of the system and thus lowers the current entropy. This is because each step in time we become more certain of our hypothesis choice. The quantity of the change in certainty can be described by the following equation
:\\	\begin{IEEEeqnarray*}{r/C/l}
		H(f(t+\Delta))  & = & H(f(t))-\Delta I(f(t))\IEEEyesnumber\label{eq:f_I_H_delta_change}
	\end{IEEEeqnarray*}
\\ The definition of the derivative entropy by $f$, is defined by
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\frac{H(f)}{df} & = & \frac{H(f(t+\Delta))-H(f(t))}{f(t+\Delta)-f(t)}
	\end{IEEEeqnarray*}
\\	which using equation (\ref{eq:f_I_H_delta_change}) yields
:\\	\begin{IEEEeqnarray*}{r?r/C/l}
		\Longrightarrow & f(t+\Delta) & = & f(t)+\frac{-\Delta I}{\frac{dH}{df}} \IEEEyesnumber\label{eq:f_change}
	\end{IEEEeqnarray*}
\\ Having developed the mutual information and the entropy's derivative we can substitute them in (\ref{eq:f_change})
:\\	\begin{IEEEeqnarray*}{r/C/l}
		f(t+\Delta) & = & f(t)+\frac{-\Delta I}{\frac{dH}{df}}
\\                  & = & f(t)+\frac{-\Delta f(t)\frac{(S_{0}-S_{1})^{2}}{\cancel{\inPiF}}\cancel{\logFrac{1+\inPiF}{1-\inPiF}}}{-\frac{\cancel{\logFrac{1-\inPiF}{1+\inPiF}}}{\cancel{\inPiF}}}
\\                  & = & f(t)-\Delta f(t)(S_{0}-S_{1})^2
\\                  & = & f(t)(1-\Delta (S_{0}-S_{1})^2)
	\end{IEEEeqnarray*}
\\ Which result in the following differential equation
:\\	\begin{IEEEeqnarray*}{r?r/C/l}
		\Rightarrow & f(t+\Delta)-f(t) & = & -\Delta f(t)(S_{0}-S_{1})^2
\\      \Rightarrow & \frac{f(t+\Delta)-f(t)}{\Delta} & = & -f(t)(S_{0}-S_{1})^2
\\		\Rightarrow & \frac{df(t)}{dt} & = & -f(t)(S_{0}-S_{1})^2
	\end{IEEEeqnarray*}
\\ which yields the following solution
\\	\begin{equation}
    	\eqbox{
    		\begin{IEEEeqnarraybox}{r/C/l}
    			f(t) & = & f(0)e^{-(S_{0}-S_{1})^2f(t)}\IEEEyesnumber\label{eq:f_explicit_solution}
    		\end{IEEEeqnarraybox}
    	}
    \end{equation}
\end{document}