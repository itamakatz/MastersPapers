\documentclass[12pt]{article}
\usepackage{lingmacros}
\usepackage{tree-dvips}
\usepackage{amsmath}
\usepackage{IEEEtrantools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{sectsty}
\usepackage{xstring}
\usepackage{xparse}

\usepackage{mleftright}
\mleftright

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}

\usepackage{cancel}

\interdisplaylinepenalty=100
% \interdisplaylinepenalty=10
% \interdisplaylinepenalty=0

\usepackage[colorlinks=true,linkcolor=blue]{hyperref}
\makeatletter
\def\IEEElabelanchoreqn#1{\bgroup
\def\@currentlabel{\p@equation\theequation}\relax
\def\@currentHref{\@IEEEtheHrefequation}\label{#1}\relax
\Hy@raisedlink{\hyper@anchorstart{\@currentHref}}\relax
\Hy@raisedlink{\hyper@anchorend}\egroup}
\makeatother
\newcommand{\subnumberinglabel}[1]{\IEEEyesnumber
\IEEEyessubnumber*\IEEElabelanchoreqn{#1}}

\usepackage[dvipsnames]{xcolor}

\usepackage[bottom=1in,top=1in]{geometry}

\graphicspath{ {./images/} }
\iffalse
	\sectionfont{\fontsize{16}{15}\selectfont}
\fi

% *** Magic Commands! *** %

	% call empty: \coherent[]{S}
	% call with index: \coherent[0]{S+l}
	\NewDocumentCommand\coherent{om}{%
		\IfNoValueTF{#1}
		{$|#2\rangle$}
		{$|#2_#1\rangle$}%
	}

    \newcommand{\logFrac}[2]{\log{\left[\frac{#1}{#2}\right]}}
	\newcommand{\logH}[1]{#1\log{\left[\frac{1}{#1}\right]}}
	\newcommand{\logHshort}[1]{#1\log{(#1)}}
	\newcommand{\binatyEntropy}[1]{\logH{#1}+\logH{(1-#1)}}

	\newcommand{\lambdaWave}{\overline{\lambda}}
	\newcommand{\curlyBracket}[1]{
		\begin{equation*}
			\left\{ \,
			\begin{IEEEeqnarraybox}[][c]{l"s}
				\IEEEstrut
					#1
				\IEEEstrut
			\end{IEEEeqnarraybox}
			\right.
		\end{equation*}
	}
	\newcommand{\curlyBracketFull}[2]{
		\begin{equation*}
			#1 \left\{ \,
			\begin{IEEEeqnarraybox}[][c]{l"s}
				\IEEEstrut
					#2
				\IEEEstrut
			\end{IEEEeqnarraybox}
			\right.
		\end{equation*}
	}
	\newcommand{\lambdaSwitch}[1]{%
		\IfEqCase{#1}{%
			{0}{\lambda_0=|S_0+l|^2}%
			{1}{\lambda_1=|S_1+l|^2}%
			{i}{\lambda_i=|S_i+l|^2}%
			{~}{\widetilde{\lambda}=\pi_{0}\sqrt{\lambda_{0}}+\pi_{1}\sqrt{\lambda_{1}}}%
			{-}{\lambdaWave=\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}}%
		}[\PackageError{tree}{Undefined option to tree: #1}{}]%
	}%

	
	\newcommand{\g}{g = \frac{\pi_{0}}{\pi_{1}}}
	\newcommand{\lStar}{\frac{S_{0}\pi_{0} - S_{1}\pi_{1}}{\pi_{1}-\pi_{0}}}
	
	\newcommand{\piG}[1]{%
		\IfEqCase{#1}{%
			{0}{\frac{g}{1+g}}%
			{1}{\frac{1}{1+g}}%
		}[\PackageError{tree}{Undefined option to tree: #1}{}]%
	}%

	\newcommand{\piDiff}[1]{%
		\IfEqCase{#1}{%
			{0}{\pi_{0}-\pi_{1}}%
			{1}{\pi_{1}-\pi_{0}}%
		}[\PackageError{tree}{Undefined option to tree: #1}{}]%
	}%

    \newcommand{\f}{f(t) = \pi_{0}(t)*\pi_{1}(t)}

	\newcommand{\piF}[1]{%
		\IfEqCase{#1}{%
			{0}{\frac{1+\sqrt{1-4f(t)}}{2}}%
			{1}{\frac{1-\sqrt{1-4f(t)}}{2}}%
		}[\PackageError{tree}{Undefined option to tree: #1}{}]%
	}%
	
	\newcommand{\inPiF}{\sqrt{1-4f(t)}}
	
	\newcommand{\sDiff}{S_{0}-S_{1}}

% *********************** %

	\def\changemargin#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
	\let\endchangemargin=\endlist 

	\newlength{\eqboxstorage}
	\newcommand{\eqbox}[1]{
		\setlength{\eqboxstorage}{\fboxsep}
		\setlength{\fboxsep}{6pt}
		\boxed{#1}
		\setlength{\fboxsep}{\eqboxstorage}
	}

	\newcommand{\dd}{\mathop{}\!\mathrm{d}}

    \newcommand{\starbreak}{%
        \fancybreak{* * *}%
    }

% *********************** %

\begin{document}

	\begin{center}
	    \sectionfont{\LARGE\selectfont}
		\section*{Signal detection over an optical channel}
	\end{center}
	\vspace{0.3cm}

\sectionfont{\Large\selectfont}

\section{Introduction}

	Given an optical channel, a signal on that channel can be described as a coherent state denoted by \coherent{S} where  $S\in\mathbb{C}$
.\\\par
	Considering a Noise free environment, in order to detect a signal in this type of channel, 
	we use a photon counter receiver. Such a receiver is a direct detection receiver which detects 
	the intensity of the optical and generates a Poisson process, where the rate of the process holds $\lambda = |S|^2$ 
.\\\par
	Suppose we have two coherent state signals denoted by \coherent[0]{S}, \coherent[1]{S} and we would like to distinguish between the two 
	binary hypotheses with the corresponding priori probabilities $\pi_0, \pi_1$ respectively under hypotheses $H=0,1$, 
	while holding some transmission cost constraint
.\\\par
	One approach was given by \textbf{Kennedy} who proposed adding a constant additional coherent state signal \coherent{l}
	before feeding the signal’s input to the receiver. Doing so generates a coherent state \coherent{S+l} which 
	the receiver in turn outputs a Poisson process with rate $\lambdaSwitch{i}$
.\\\par
	An additional approach was given by \textbf{Dolinar} who suggested as continuation to Kennedy’s design to replace the constant 
	signal with a controlled signal \coherent{l(t)} which is chosen adaptively based on the photon arrivals up to that moment, 
	in order to achieve more certainty in the hypothesis choice with time
.\\\par
	The core concept of Dolinar’s updated design includes a recursive method in which the posterior probabilities of 
	the two possible hypotheses denoted by $\pi_1(t), \pi_2(t)$ are updated after each step of time $\Delta$ to yield 
	$\pi_1(t+\Delta), \pi_2(t+\Delta)$
.\\\par
At this point, by making $\Delta$ arbitrarily small we can expect the current Poisson process to return
:\\	\curlyBracket{
		0 & w.p. $(1-\lambda_i\Delta)$
\\		1 & w.p. $(\lambda_i\Delta)$
	}
Which can be thought of as the following binary channel (Figure \ref{fig:channel})
:\pagebreak[2]
	\begin{figure}[H]
		\centering
		\includegraphics[width=8cm]{channel.png}
		\caption{Equivalent binary channel over time $\Delta$}
		\label{fig:channel}
	\end{figure}
\section{Optimal local signal}	
Since we obtained an approximation of a binary channel we may now ask how should $l(t)$ be decided in 
other to maximize the mutual information of the binary channel
.\\\par	The Entropy can be calculated as follows
:\\	\begin{IEEEeqnarray*}{r/C/l}
		H(Y) 	& = & H_{b}[\Delta(\pi_{0}\lambda_{0} + \pi_{1}\lambda_{1})]
\\		H(Y|X) 	& = & \pi_{0}H_{b}(\Delta\lambda_{1}) + \pi_{1}H_{b}(\Delta\lambda_{1})
	\end{IEEEeqnarray*}
\\	Which gives us the mutual information of the channel
:\\	\begin{IEEEeqnarray*}{r/C/l}
		I(X;Y) 	& = & H(Y)-H(Y|X)
\\				& = & H_{b}[\Delta(\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1})]-\left[\pi_{0}H_{b}(\Delta\lambda_{1})+\pi_{1}H_{b}(\Delta\lambda_{1})\right]
		\label{eq:dont_use_multline}
	\end{IEEEeqnarray*}
\\	Since we want to maximize the mutual information, we shall compare its derivative to zero
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\frac{dI(X;Y)}{dl} 	& = & \log_{2}\left[\frac{1-\Delta[\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}]}{\Delta[\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}]}\right]2\Delta(\pi_{0}\sqrt{\lambda_{0}}+\pi_{1}\sqrt{\lambda_{1}})
\\							&   & -\> \log_{2}\left[\frac{1-\lambda_{0}\Delta}{\lambda_{0}\Delta}\right]2\Delta\pi_{0}\sqrt{\lambda_{0}}
\\							&   & -\> \log_{2}\left[\frac{1-\lambda_{1}\Delta}{\lambda_{1}\Delta}\right]2\Delta\pi_{1}\sqrt{\lambda_{1}}
\\							& = & 0
	\end{IEEEeqnarray*}
	\begin{IEEEeqnarray*}{r/C/l}
		\frac{dI(X;Y)}{dl} 	& = & \log_{2}\left[\frac{1-\Delta[\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}]}{\Delta[\pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}]}\right]2\Delta(\pi_{0}\sqrt{\lambda_{0}}+\pi_{1}\sqrt{\lambda_{1}})
\\							&   & -\> 2\Delta\left[\log_{2}\left(\frac{1-\lambda_{0}\Delta}{\lambda_{0}\Delta}\right)\pi_{0}\sqrt{\lambda_{0}}+\log_{2}\left(\frac{1-\lambda_{1}\Delta}{\lambda_{1}\Delta}\right)\pi_{1}\sqrt{\lambda_{1}}\right]
\\							& = & 0
	\end{IEEEeqnarray*}
\\	The optimal choice for $l$ is
:\\	\begin{equation*}
		\eqbox{
			l^{*} = \lStar
		}
	\end{equation*}
\\	This optimal solution is numerically proven to be correct. The analytical proof will be included in a future paper.
	
% \vspace{5mm}	
% \centerline{* * *}
% \vspace{5mm}

\section{Explicit equation for the control system in the optimal case}

\subsection{Implication of the optimal case}

In the optimal case where $l=l^{*}=\lStar$, we can calculate the following expressions for the $\lambda$'s
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\lambda_{i} & = & (l^{*}+S_{i})^{2}
\\					& = & \left(\lStar+S_{i}\right)^{2}
\\					& = & \left(\frac{S_{0}\pi_{0} - S_{1}\pi_{1}+S_{i}(\piDiff{1})}{\piDiff{1}}\right)^{2}
	\end{IEEEeqnarray*}
\\	Which yields:
\\	\begin{IEEEeqnarray*}{r/C/l}
		\overline{\lambda} 	& = & \pi_{0}\lambda_{0}+\pi_{1}\lambda_{1}
\\							& = & \pi_{0}\left(\frac{\pi_{1}(\sDiff)}{\piDiff{1}}\right)^{2}+\pi_{1}\left(\frac{\pi_{0}(\sDiff)}{\piDiff{1}}\right)^{2}
\\							& = & (\pi_{0}\pi_{1}^{2}+\pi_{1}\pi_{0}^{2})\left(\frac{(\sDiff)}{\piDiff{1}}\right)^{2}\IEEEyesnumber\label{eq:before_lambda_wave}
	\end{IEEEeqnarray*}
\\	Since $\pi_{0}+\pi_{1}=1$ we have that
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\pi_{0}\pi_{1}^{2}+\pi_{1}\pi_{0}^{2} 	& = & \pi_{0}\pi_{1}(1-\pi_{0}) +\pi_{1}\pi_{0}^{2}
\\												& = & \pi_{0}\pi_{1} - \pi_{0}\pi_{1}^{2} + \pi_{0}\pi_{1}^{2}
\\												& = & \pi_{0}\pi_{1}
	\end{IEEEeqnarray*}
\\	Thus we can substitute in (\ref{eq:before_lambda_wave}) and get
:\\	\begin{IEEEeqnarray*}{r/C/l}
		(\pi_{0}\pi_{1}^{2}+\pi_{1}\pi_{0}^{2})\left(\frac{\sDiff}{\piDiff{1}}\right)^{2} & = & \pi_{0}\pi_{1}\left(\frac{\sDiff}{\piDiff{1}}\right)^{2}
	\end{IEEEeqnarray*}
\\	Overall we get the following expression for the $\lambda$'s
:\\	\curlyBracket{
		\lambda_{0} = \left(\frac{\pi_{1}(\sDiff)}{\piDiff{1}}\right)^{2}
\\		\lambda_{1} = \left(\frac{\pi_{0}(\sDiff)}{\piDiff{1}}\right)^{2}
\\		\overline{\lambda} = \pi_{0}\pi_{1}\left(\frac{\sDiff}{\piDiff{1}}\right)^{2}
	}
	
\vspace{2mm}

Before deriving the mutual information in the optimal case, we would like to point out the following observation.

Since $\Delta\rightarrow 0$, A binary entropy function containing a $\Delta$ as part of its argument,
will have only one relevant part of the entropy definition, since $z\log(z)=0$ if $z\rightarrow 1$. Formally, for some bound $z\in\mathbb{R}$ and $\Delta\rightarrow 0$ we have that
:\\	\begin{IEEEeqnarray*}{r/C/l}
	H_{b}(\Delta z) & = & \binatyEntropy{\Delta z}
\\					& \simeq & \logH{\Delta z}
\end{IEEEeqnarray*}
\subsection{The mutual information}
Using this observation we can now go on developing the mutual information
:\\	\begin{IEEEeqnarray*}{r/C/l}
		I 	& = 		& H(Y) - H(Y|X)
\\			& = 		& H_{b}[\lambdaWave\Delta]-\pi_{0}H_{b}(\lambda_{0}\Delta)-\pi_{1}H_{b}(\lambda_{1}\Delta)
\\			& \simeq 	& -\> \logHshort{\lambdaWave\Delta}+\pi_{0}\logHshort{\lambda_{0}\Delta}+\pi_{1}\logHshort{\lambda_{1}\Delta}
\\			& = 		& -\> \log(\Delta)\left[\cancel{-\lambdaWave+\lambda_{0}\pi_{0}+\lambda_{1}\pi_{1}}\right]+\Delta(-\logHshort{\lambdaWave}+\pi_{0}\logHshort{\lambda_{0}}+\pi_{}\logHshort{\lambda_{1}})
\\			& = 		& \Delta(-\logHshort{\lambdaWave}+\pi_{0}\logHshort{\lambda_{0}}+\pi_{}\logHshort{\lambda_{1}})
\\			& = 		& \Delta\logH{\pi_{0}\pi_{1}\left(\frac{\sDiff}{\piDiff{1}}\right)^{2}}
\\			&			&		-\> \pi_{0}\logH{\left(\frac{\pi_{1}(\sDiff)}{\piDiff{1}}\right)^{2}}
\\			&			&		-\> \pi_{1}\logH{\left(\frac{\pi_{0}(\sDiff)}{\piDiff{1}}\right)^{2}}
\\			& = 		& \Delta\pi_{0}\pi_{1}\left(\frac{\sDiff}{\piDiff{1}}\right)^{2}
\\			&			&		\left( 	- \log{\left[\pi_{0}\pi_{1}\left((\frac{(\sDiff)}{\piDiff{1}})\right)^{2}\right]}		\right.
\\			&			&				+\> \pi_{0}\log{\left[\left(\frac{\pi_{1}(\sDiff)}{\piDiff{1}})\right)^{2}\right]}
\\			&			&		\left. 	+\> \pi_{1}\log{\left[\left(\frac{\pi_{0}(\sDiff)}{\piDiff{1}})\right)^{2}\right]}	\right)
\\			& = 		& \Delta\pi_{0}\pi_{1}\left(\frac{\sDiff}{\piDiff{1}}\right)^{2}
\\			&			&		\left(	- \log{[\pi_{0}\pi_{1}]} - \cancel{\log{\left[\left((\frac{(\sDiff)}{\piDiff{1}})\right)^{2}\right]}}				\right.
\\			&			&				+\> 2\pi_{0}\log{[\pi_{0}]} + \cancel{\pi_{0}\log{\left[\left(\frac{(\sDiff)}{\piDiff{1}})\right)^{2}\right]}}
\\			&			&		\left.	+\> 2\pi_{1}\log{[\pi_{1}]} + \cancel{\pi_{1}\log{\left[\left(\frac{(\sDiff)}{\piDiff{1}})\right)^{2}\right]}} 	\right)
\\			& = 		& \Delta\pi_{0}\pi_{1}\left(\frac{\sDiff}{\piDiff{1}}\right)^{2} ( - log{[\pi_{0}\pi_{1}]} + 2\pi_{0}\log{[\pi_{0}]} + 2\pi_{1}\log{[\pi_{1}]})
\\			& = 		& \Delta\pi_{0}\pi_{1}\left(\frac{\sDiff}{\piDiff{1}}\right)^{2} ( - log{[\pi_{0}]} - log{[\pi_{1}]} + 2\pi_{0}\log{[\pi_{0}]} + 2\pi_{1}\log{[\pi_{1}]})
\\			& = 		& \Delta\pi_{0}\pi_{1}\left(\frac{\sDiff}{\piDiff{1}}\right)^{2} ( (2\pi_{0}-1)log{[\pi_{0}]} + (2\pi_{1}-1)log{[\pi_{1}]}) \IEEEyesnumber\label{eq:before_two_pi}
	\end{IEEEeqnarray*}
\\	Again, since $\pi_{0}+\pi_{1}=1$ we have
:\\	\curlyBracket{
		2\pi_{0}-1=\pi_{0}+\pi_{0}-1=\piDiff{0}
\\		2\pi_{1}-1=\pi_{1}+\pi_{1}-1=\piDiff{1}
	}
\\	So we can substitute in (\ref{eq:before_two_pi}) and get
:\\	\begin{IEEEeqnarray*}{r/C/l}
		& = & \Delta\pi_{0}\pi_{1}\frac{(\sDiff)^{2}}{(\piDiff{1})^{\cancel{2}}}\cancel{(\piDiff{0})}(log{[\pi_{0}]}-log{[\pi_{1}]})
\\		& = & -\> \Delta\pi_{0}\pi_{1}\left(\frac{(\sDiff)^{2}}{\piDiff{1}}\right) log{\left[\frac{\pi_{0}}{\pi_{1}}\right]}
\\		& = & \Delta\pi_{0}\pi_{1}\left(\frac{(\sDiff)^{2}}{\piDiff{0}}\right) log{\left[\frac{\pi_{0}}{\pi_{1}}\right]}\IEEEyesnumber\label{eq:before_f}
	\end{IEEEeqnarray*}
	
\subsection{Parametrization function of the probabilities}

At this point we shall parameterize our probabilities with parameter $\f$ which will let us describe the recursive procedure in the system. That definition yields the parametrization
:\\	\curlyBracketFull
    {\f \Longrightarrow}
    {
		\pi_{0}(t)=\piF{0}
\\		\pi_{1}(t)=\piF{1}
	}
	
With this new parameterization, we can now give an alternative continuation developing the Mutual Information we found in equation (\ref{eq:before_f})
:\\	\begin{IEEEeqnarray*}{r/C/l}
        I   & = & \pi_{0}\pi_{1}\left(\frac{(\sDiff)^{2}}{\piDiff{0}}\right) log{\left[\frac{\pi_{0}}{\pi_{1}}\right]}
\\          & = & f(t)\frac{(\sDiff)^{2}}{\inPiF}\logFrac{1+\inPiF}{1-\inPiF}
	\end{IEEEeqnarray*}
	
As we did with the mutual information, we can now develop the Entropy of the channel and substitute with our current parameter of interest $f(t)$
:\\	\begin{IEEEeqnarray*}{r/C/l}
        H_b(\pi_0,\pi_1)    & = & \logH{\pi_0}+\logH{\pi_1}
\\                          & = & \logH{\pi_1}+(1-\pi_0)\logFrac{1}{\pi_1}
\\                          & = & \logFrac{1}{\pi_1}+\pi_0\logFrac{\pi_1}{\pi_0}
\\                          & = & \logFrac{2}{1-\inPiF}+\frac{1+\inPiF}{2}\logFrac{1-\inPiF}{1+\inPiF}
	\end{IEEEeqnarray*}
\\ This yields the following derivative of the entropy
:\\	\begin{IEEEeqnarray*}{r/C/l}
        \frac{dH(f(t))}{df(t)}    & = & -\frac{\logFrac{1-\inPiF}{1+\inPiF}}{\inPiF}
	\end{IEEEeqnarray*}
	
\pagebreak[2]

Each step of the recursion, the current mutual information gives us more knowledge of the system and thus lowers the current entropy. This is because each step in time we become more certain of our hypothesis choice. The quantity of the change in certainty can be described by the following equation
:\\	\begin{IEEEeqnarray*}{r/C/l}
		H(f(t+\Delta))  & = & H(f(t))-\Delta I(f(t))\IEEEyesnumber\label{eq:f_I_H_delta_change}
	\end{IEEEeqnarray*}
\\ The definition of the derivative entropy by $f$, is defined by
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\frac{H(f)}{df} & = & \frac{H(f(t+\Delta))-H(f(t))}{f(t+\Delta)-f(t)}
	\end{IEEEeqnarray*}
\\	which using equation (\ref{eq:f_I_H_delta_change}) yields
:\\	\begin{IEEEeqnarray*}{r?r/C/l}
		\Longrightarrow & f(t+\Delta) & = & f(t)+\frac{-\Delta I}{\frac{dH}{df}} \IEEEyesnumber\label{eq:f_change}
	\end{IEEEeqnarray*}
\\ Having developed the mutual information and the entropy's derivative we can substitute them in (\ref{eq:f_change})
:\\	\begin{IEEEeqnarray*}{r/C/l}
		f(t+\Delta) & = & f(t)+\frac{-\Delta I}{\frac{dH}{df}}
\\                  & = & f(t)+\frac{-\Delta f(t)\frac{(\sDiff)^{2}}{\cancel{\inPiF}}\cancel{\logFrac{1+\inPiF}{1-\inPiF}}}{-\frac{\cancel{\logFrac{1-\inPiF}{1+\inPiF}}}{\cancel{\inPiF}}}
\\                  & = & f(t)-\Delta f(t)(\sDiff)^2
\\                  & = & f(t)(1-\Delta (\sDiff)^2)
	\end{IEEEeqnarray*}
\\ Which result in the following differential equation
:\\	\begin{IEEEeqnarray*}{r?r/C/l}
		\Rightarrow & f(t+\Delta)-f(t) & = & -\Delta f(t)(\sDiff)^2
\\      \Rightarrow & \frac{f(t+\Delta)-f(t)}{\Delta} & = & -f(t)(\sDiff)^2
\\		\Rightarrow & \frac{df(t)}{dt} & = & -f(t)(\sDiff)^2
	\end{IEEEeqnarray*}
\\ which yields the solution
\\	\begin{equation}
    	\eqbox{
    		\begin{IEEEeqnarraybox}{r/C/l}
    			f(t) & = & f(0)e^{-(\sDiff)^2f(t)}\IEEEyesnumber\label{eq:f_explicit_solution}
    		\end{IEEEeqnarraybox}
    	}
	\end{equation}
	
% \vspace{5mm}	
% \centerline{* * * * *}
% \vspace{5mm}
\pagebreak

\section{Alternative derivation}
\subsection{Original proposed parametrization}

It is worth mentioning the original parametrization proposed in the Chung Capacity paper. Thus the following section will develop the solution derived there.
\linebreak

We shall parameterize our probabilities with parameter $\g$ which will let us describe the recursive procedure in the system. That definition yields the parametrization
:\\	\curlyBracketFull
    {\g \Longrightarrow}
    {
		\pi_{0}=\piG{0}
\\		\pi_{1}=\piG{1}
	}
\\	With that parameterization, we can now continue developing (\ref{eq:before_f})
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\pi_{0}\pi_{1}\left(\frac{(S_{0}-S_{1})^{2}}{\pi_{0}-\pi_{1}}\right) log{\left[\frac{\pi_{0}}{\pi_{1}}\right]}
			& = & (\piG{0})(\piG{1})\frac{(S_{0}-S_{1})^{2}}{\piG{0}-\piG{1}}log{\left(\frac{\piG{0}}{\piG{1}}\right)}
\\			& = & \left(\frac{g}{(1+g)^{\cancel{2}}}\right)\left(\frac{(S_{0}-S_{1})^{2}}{\frac{g-1}{\cancel{1+g}}}\right)\log{(g)}
\\			& = & \frac{g(S_{0}-S_{1})^{2}}{(1-g)(1+g)}\log{(g)}
	\end{IEEEeqnarray*}
\\	We can also recompute the entropy using this parameterization
:\\	\begin{IEEEeqnarray*}{r/C/l}
		H(g) 	& = & H_{b}(\pi_{0},\pi_{1})
\\				& = & \logH{\pi_{0}}+\logH{\pi_{1}}
\\				& = & \logH{\piG{0}}+\logH{\piG{1}}
\\				& = & \> - \piG{1}\left(g\log{\piG{0}}+\log{\piG{1}}\right)
\\				& = & \> - \piG{1}\left(g\log{g}+(1+g)\log{\piG{1}}\right)
\\				& = & \> \piG{1}\left(\logHshort{(1+g)}-\logHshort{g}\right)
\\				& = & \> \log{(1+g)}-\piG{0}\log{g}
	\end{IEEEeqnarray*}
\\	So we can now find the derivative of the entropy as a function of $g$
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\frac{H(g)}{dg} & = & \frac{d(\log{(1+g)}-\piG{0}\log{g})}{dg}
\\						& = & \piG{1}-\frac{(1+\cancel{g})-\cancel{g}}{(1+g)^{2}}\log{g}-\frac{1}{\cancel{g}}\frac{\cancel{g}}{\left(1+g\right)}
\\						& = & \cancel{\piG{1}}-\frac{\log{g}}{(1+g)^{2}}-\cancel{\piG{1}}
\\						& = & \> - \frac{\log{g}}{(1+g)^{2}}
	\end{IEEEeqnarray*}
\\	Each step of the recursion, the entropy is lowered since we have more certainty of our choice. The quantity of the change in certainty can be describe by the following equation
:\\	\begin{IEEEeqnarray*}{r/C/l}
		H(g(t+\Delta)) & = & H(g(t))-\Delta I(g(t))\IEEEyesnumber\label{eq:I_H_delta_change}
	\end{IEEEeqnarray*}
\\	If we look again at the the derivative of the entropy, we see than indeed it is negative which coincides with the fact that the certainty is increasing
.\\\par	The definition of the derivative entropy by $g$, is defined by
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\frac{H(g)}{dg} & = & \frac{H(g(t+\Delta))-H(g(t))}{g(t+\Delta)-g(t)}
	\end{IEEEeqnarray*}
\\	which yields
:\\	\begin{IEEEeqnarray*}{r?r/C/l}
		\Longrightarrow & g(t+\Delta) & = & g(t)+\frac{- \Delta I}{\frac{dH}{dg}} \IEEEyesnumber\label{eq:g_change}
	\end{IEEEeqnarray*}
\\	Since we found already the derivative and the mutual information, we can substitute them in (\ref{eq:g_change})
:\\	\begin{IEEEeqnarray*}{r/C/l}
		g(t+\Delta) & = & g(t)+\frac{-\Delta I}{\frac{dH}{dg}}
\\					& = & g(t)+\frac{-\Delta\frac{g(S_{0}-S_{1})^{2}}{(1-g)\cancel{(1+g)}}\cancel{\log{(g)}}}{-\frac{\cancel{\log{(g)}}}{(1+g)^{\cancel{2}}}}
\\					& = & g(t)+\left(\frac{\Delta g(1+g)}{(1-g)}\right)(S_{0}-S_{1})^{2}
\end{IEEEeqnarray*}
Or
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\frac{g(t+\Delta)}{g(t)} & = & 1+\left(\frac{\Delta(1+g)}{(1-g)}\right)(S_{0}-S_{1})^{2}\IEEEyesnumber\label{eq:g_delta_ratio}
\end{IEEEeqnarray*}
\\	By Tailor approximations, for $x<<1$ it holds that $\log{(1+x)} \simeq x-\frac{1}{2}x^{2}$. So by setting $x=\left(\frac{\Delta(1+g)}{(1-g)}\right)(S_{0}-S_{1})^{2}$ we have that
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\log{\left[1 + \left(\frac{\Delta(1+g)}{(1-g)}\right)(S_{0}-S_{1})^{2}\right]} & = & \log{\left[\frac{g(t+\Delta)}{g(t)}\right]}
\\		& = & \log{\left[1+\frac{g(t+\Delta)-g(t)}{g(t)}\right]}
\\													& \simeq & \frac{g(t+\Delta)-{g(t)}}{g(t)}\IEEEyesnumber\label{eq:tailor_approximation}
\end{IEEEeqnarray*}
\\	Applying (\ref{eq:tailor_approximation}) to equation (\ref{eq:g_delta_ratio}) and assuming that $S_{0}(t)$ and $S_{1}(t)$ are constant in time and therefore we can omit their dependency on $t$ we get that
:\\	\begin{IEEEeqnarray*}{r/C/l}
       1+\left(\frac{\Delta(1+g)}{(1-g)}\right)(S_{0}-S_{1})^{2} & = & \rm{exp}\left[\frac{g(t+\Delta)-{g(t)}}{g(t)}\right]
\\    	g(t) 	& = & g(0) \cdot{\rm{exp}\left[\int_{0}^{t} (\frac{(S_{0}-S_{1})^{2}(g(\tau)+1)}{g(\tau)-1}) \dd{\tau} \right]}
\\				& = & g(0) \cdot{\prod_{\tau=0}^{t} \rm{exp}\left[\frac{(S_{0}-S_{1})^{2}(g(\tau)+1)}{g(\tau)-1} \right]}
\end{IEEEeqnarray*}
\\	We can now look at the change ratio
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\frac{g(t+\Delta)}{g(t)}	& = & \frac	{g(0) \cdot{\prod_{\tau=0}^{t+\Delta} \rm{exp} \left[\frac{(S_{0}-S_{1})^{2}(g(\tau)+1)}{g(\tau)-1} \right]}}
												{g(0) \cdot{\prod_{\tau=0}^{t} \rm{exp} \left[\frac{(S_{0}-S_{1})^{2}(g(\tau)+1)}{g(\tau)-1} \right]}}
\\									& = & \rm{exp} \left[\frac{(S_{0}-S_{1})^{2}(g(\tau+\Delta)+1)}{g(\tau+\Delta)-1} \right]
\end{IEEEeqnarray*}
\\	Which gives
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\log{\left[\frac{g(t+\Delta)}{g(t)}\right]} & = & \frac{(S_{0}-S_{1})^{2}(g(\tau+\Delta)+1)}{g(\tau+\Delta)-1}\IEEEyesnumber\label{eq:derived_from_exp}
\end{IEEEeqnarray*}
\\	Substituting  that in (\ref{eq:derived_from_exp}) gives
:\\	\begin{IEEEeqnarray*}{r/C/l}
		\frac{g(t+\Delta)-{g(t)}}{g(t)} & \simeq & \frac{(S_{0}-S_{1})^{2}(g(\tau+\Delta)+1)}{g(\tau+\Delta)-1}
\end{IEEEeqnarray*}
\\	Which is what we got to.

\end{document}